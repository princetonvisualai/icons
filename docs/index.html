<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="ICONS: Influence Consensus for Vision-Language Data Selection">
  <meta name="keywords" content="ICONS, Data Selection, Vision-Language, LLaVA">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ICONS: Influence Consensus for Vision-Language Data Selection</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">

  <style>
    body {
      font-family: 'Google Sans', sans-serif;
      margin: 0;
      padding: 0;
      line-height: 1.6;
    }

    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 20px;
    }

    .title-section {
      text-align: center;
      padding: 40px 20px;
      background: #e9ecef;
    }

    .title {
      font-size: 2.5em;
      margin-bottom: 10px;
    }

    .authors {
      font-size: 1.2em;
      margin: 20px 0;
    }

    .author-block {
      margin: 0 10px;
    }

    .author-block a {
      color: #0056b3;
      text-decoration: none;
    }

    .author-block a:hover {
      text-decoration: underline;
    }

    .publication-links {
      margin: 20px 0;
    }

    .button {
      display: inline-block;
      padding: 10px 20px;
      margin: 5px;
      border-radius: 5px;
      background: #0056b3;
      color: white;
      text-decoration: none;
      transition: background 0.3s;
    }

    .button:hover {
      background: #003d80;
    }

    .content-section {
      padding: 40px 20px;
    }

    .figure-container {
      text-align: center;
      margin: 20px 0;
    }

    .figure-container img {
      max-width: 100%;
      height: auto;
    }

    .caption {
      font-style: italic;
      color: #666;
      margin-top: 10px;
    }

    .section-title {
      font-size: 1.8em;
      margin: 40px 0 20px;
      color: #333;
    }

    .abstract {
      background: #e9ecef;
      padding: 20px;
      border-radius: 5px;
      margin: 20px 0;
    }

    .method-overview {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;
      margin: 20px 0;
    }

    .method-step {
      flex: 1;
      min-width: 300px;
      background: #fff;
      padding: 20px;
      border-radius: 5px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .citation-box {
      background: #f8f9fa;
      padding: 20px;
      border-radius: 5px;
      margin: 20px 0;
      font-family: 'Consolas', monospace;
      white-space: pre-wrap;
      overflow-x: auto;
    }

    .acknowledgments {
      background: #e9ecef;
      padding: 20px;
      border-radius: 5px;
      margin: 20px 0;
    }

    /* Custom Hugging Face icon */
    .fa-huggingface::before {
      content: "ðŸ¤—";  /* Using the hugging face emoji as a simple solution */
    }

    .icon-huggingface {
      background-image: url('https://huggingface.co/front/assets/huggingface_logo.svg');
      background-size: contain;
      background-repeat: no-repeat;
      width: 24px;
      height: 24px;
      display: inline-block;
      vertical-align: middle;
    }

    .huggingface-small {
      font-size: 14px;
      vertical-align: middle;
    }
  </style>
</head>

<body>
  <div class="title-section">
    <div class="container">
      <h1 class="title">ICONS: Influence Consensus for Vision-Language Data Selection</h1>
      <div class="authors">
        <span class="author-block"><a href="https://xindiwu.github.io/">Xindi Wu</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://xiamengzhou.github.io/">Mengzhou Xia</a><sup>1</sup>,</span>
        <span class="author-block"><a href="https://rulinshao.github.io/">Rulin Shao</a><sup>2</sup>,</span>
        <span class="author-block"><a href="https://lucas2012.github.io/">Zhiwei Deng</a><sup>3</sup>,</span>
        <span class="author-block"><a href="https://koh.pw/">Pang Wei Koh</a><sup>2,4</sup>,</span>
        <span class="author-block"><a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a><sup>1</sup></span>
      </div>
      <div class="affiliations">
        <span>1. Princeton University</span> |
        <span>2. University of Washington</span> |
        <span>3. Google DeepMind</span> |
        <span>4. Allen Institute for AI</span>
      </div>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2501.00654" class="button">
          <i class="ai ai-arxiv"></i> Paper
        </a>
        <a href="https://github.com/princetonvisualai/icons" class="button">
          <i class="fab fa-github"></i> Code
        </a>
        <a href="https://huggingface.co/datasets/xdiwu/LLaVA-ICONS-133K" class="button">
          <span class="huggingface-small">ðŸ¤—</span> Dataset
        </a>
        <p style="margin-top: 15px; color: #666;">Website under construction ðŸš§</p>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="content-section">
      <div class="abstract">
        <h2>Abstract</h2>
        <p>Visual Instruction Tuning typically requires a large amount of vision-language training data. This data often contains redundant information that increases computational costs without proportional performance gains.</p>
        
        <div style="display: flex; justify-content: space-between; gap: 20px; margin: 20px 0;">
          <div style="flex: 1; background: #f8f9fa; padding: 15px; border-radius: 5px; min-width: 0;">
            <h3 style="margin: 0 0 10px 0; font-size: 1.1em;">
              <i class="fas fa-lightbulb" style="margin-right: 8px;"></i>Approach
            </h3>
            <p style="margin: 0;">The key element of our approach is cross-task influence consensus, which uses majority voting across task-specific influence matrices to identify samples that are consistently valuable across multiple tasks, allowing us to effectively prioritize data that optimizes for overall performance.</p>
          </div>
          
          <div style="flex: 1; background: #f8f9fa; padding: 15px; border-radius: 5px; min-width: 0;">
            <h3 style="margin: 0 0 10px 0; font-size: 1.1em;">
              <i class="fas fa-chart-line" style="margin-right: 8px;"></i>Experiments
            </h3>
            <p style="margin: 0;">Experiments show that models trained on our selected data (20% of LLAVA-665K) achieve 98.6% of the relative performance obtained using the full dataset, and exceeds 102% of the full dataset performance at 60% selection ratio.</p>
          </div>
          
          <div style="flex: 1; background: #f8f9fa; padding: 15px; border-radius: 5px; min-width: 0;">
            <h3 style="margin: 0 0 10px 0; font-size: 1.1em;">
              <i class="fas fa-database" style="margin-right: 8px;"></i>Artifact
            </h3>
            <p style="margin: 0;">We release this subset, LLAVA-ICONS-133K, a compact yet highly informative subset of LLAVA-665K visual instruction tuning data, preserving high impact training data for efficient vision-language model development.</p>
          </div>
        </div>
      </div>


      <h2 class="section-title">Method</h2>
      <div class="method-overview">
        <div class="method-step">
          <h3>Stage 1: Specialist</h3>
          <p>Computes task-specific influence scores through gradient-based analysis for each target task independently.</p>
        </div>
        <div class="method-step">
          <h3>Stage 2: Generalist</h3>
          <p>Implements consensus mechanism to identify training samples that show consistent positive value across multiple tasks.</p>
        </div>
      </div>

      <div class="figure-container">
        <img src="static/images/pipeline.png" alt="ICONS Pipeline">
        <p class="caption">Overview of ICONS: Our two-stage approach combines specialist task-specific influence analysis with generalist consensus-based selection.</p>
      </div>

      <h2 class="section-title">Experiments</h2>
      <div class="content-section">
        <!-- Overview Stats -->
        <div style="display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 30px;">
          <div style="flex: 1; min-width: 300px; background: #f8f9fa; padding: 20px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <h3 style="margin-top: 0;">Key Performance Highlights</h3>
            <ul style="margin: 0; padding-left: 20px;">
              <li>98.6% relative performance using only 20% of training data</li>
              <li>102% relative performance at 60% data selection</li>
              <li>Effective across 10+ vision-language benchmarks</li>
              <li>Strong generalization to unseen tasks and architectures</li>
            </ul>
          </div>
          
          <div style="flex: 1; min-width: 300px; background: #f8f9fa; padding: 20px; border-radius: 5px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
            <h3 style="margin-top: 0;">Evaluation Setup</h3>
            <ul style="margin: 0; padding-left: 20px;">
              <li>Dataset: LLaVA-665K for visual instruction tuning</li>
              <li>Base Model: LLaVA-v1.5-7b-lora</li>
              <li>Benchmarks: VQAv2, GQA, VizWiz, SQA-I, TextVQA, POPE, MME, MMBench (en/cn), LLaVA-Bench</li>
              <li>Baselines: Random, CLIP-Score, EL2N, Perplexity, SemDeDup, D2-Pruning, Self-Sup, Self-Filter, COINCIDE</li>
            </ul>
          </div>
        </div>

        <!-- Key Findings -->
        <div style="margin-bottom: 30px;">
          <h3>Key Findings</h3>
          
          <!-- Data Overlap Figure -->


          <div style="display: flex; flex-wrap: wrap; gap: 20px;">
            <div style="flex: 1; min-width: 250px; background: #fff; padding: 15px; border-radius: 5px; border: 1px solid #dee2e6;">
              <h4 style="margin-top: 0; color: #0056b3;">From Specialist to Generalist</h4>
              <p style="margin: 0;">Consensus across task-specific influence patterns identifies a compact, high-performing universal training set:</p>
              <ul style="margin-top: 10px;">
                <li>Only 1.33% average performance drop vs. specialist baselines</li>
                <li>Some tasks improve under generalist selection (SQA-I: +1.43%, POPE: +1.04%)</li>
                <li>Data overlap varies by task complexity: from 3.27% (VQAv2) to 24.21% (LLAVA-W Bench)</li>
              </ul>
              <div style="margin-top: 15px; text-align: center;">
                <img src="static/images/6_data_overlap.png" alt="Data overlap between specialist and generalist selections" style="width: 100%; max-width: 400px; height: auto;">
                <p style="color: #666; font-size: 0.9em; margin-top: 5px;">Data overlap between specialist and generalist selections across tasks</p>
              </div>
            </div>
            
            <div style="flex: 1; min-width: 250px; background: #fff; padding: 15px; border-radius: 5px; border: 1px solid #dee2e6;">
              <h4 style="margin-top: 0; color: #0056b3;">Selection Ratio Impact</h4>
              <p style="margin: 0;">Performance across different data selection ratios:</p>
              <ul style="margin-top: 10px;">
                <li>Stronger performance compared with baselines in low-selection regime (5-20%)</li>
                <li>Exceeds 100% relative performance at 60% selection</li>
              </ul>
              <div style="margin-top: 15px; text-align: center;">
                <img src="static/images/3_progressive_line.png" alt="Progressive line plot showing selection ratio impact" style="width: 100%; max-width: 400px; height: auto;">
                <p style="color: #666; font-size: 0.9em; margin-top: 5px;">Progressive performance across selection ratios</p>
              </div>
            </div>
            
            <div style="flex: 1; min-width: 250px; background: #fff; padding: 15px; border-radius: 5px; border: 1px solid #dee2e6;">
              <h4 style="margin-top: 0; color: #0056b3;">Cross-Task Influence Patterns</h4>
              <p style="margin: 0;">Pairwise overlap analysis reveals:</p>
              <ul style="margin-top: 10px;">
                <li>High overlap in related tasks:
                  <ul>
                    <li>MMBench (en-cn): 67.4%</li>
                    <li>POPE-GQA: 60.2%</li>
                    <li>VQAv2-VizWiz: 49.0%</li>
                  </ul>
                </li>
                <li>Low overlap between dissimilar tasks (e.g., MMBench-GQA: 3.3%)</li>
                <li>Findings support using influence consensus for effective multi-task selection</li>
              </ul>
              <div style="margin-top: 15px; text-align: center;">
                <img src="static/images/9_benchmark_heatmap.png" alt="Benchmark performance heatmap" style="width: 100%; max-width: 400px; height: auto;">
                <p style="color: #666; font-size: 0.9em; margin-top: 5px;">Pairwise overlap heatmap across tasks</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      <h2 class="section-title">Citation</h2>
      <div class="citation-box">@article{wu2024icons,
    title={ICONS: Influence Consensus for Vision-Language Data Selection},
    author={Wu, Xindi and Xia, Mengzhou and Shao, Rulin and Deng, Zhiwei and Koh, Pang Wei and Russakovsky, Olga},
    journal={arXiv preprint arXiv:2501.00654},
    year={2024}
}</div>

      <h2 class="section-title">Acknowledgments</h2>
      <div class="acknowledgments">
        <p>This material is based upon work supported by the National Science Foundation under Grant No. 2107048 and No.2112562. Any opinions, findings, and conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>
        <p>This work is also supported by the Singapore National Research Foundation and the National AI Group in the Singapore Ministry of Digital Development and Information under the AI Visiting Professorship Programme (award number AIVP-2024-001).</p>
        <p>We thank many people for their helpful discussion and feedback, listed in alphabetical order by last name: Allison Chen, Hamish Ivison, Carlos E. Jimenez, Polina Kirichenko, Jaewoo Lee, Tiffany Ling, Zhiqiu Lin, Ethan Tseng, Shengbang Tong, Justin Wang, Zirui Wang.</p>
      </div>
    </div>
  </div>
</body>
</html>